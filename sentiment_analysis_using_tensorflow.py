# -*- coding: utf-8 -*-
"""Sentiment-Analysis-using-TensorFlow

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17sJU3BABbQ55fEms3R6k760NaSGbPsSg
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/Ayeshasaif13/Sentiment-Analysis-using-TensorFlow.git

# %cd Sentiment-Analysis-using-TensorFlow

!pip install -r requirements.txt

from google.colab import drive
import os

os.listdir()

import pandas as pd

with open('Sentences_75Agree_sample.txt', 'r', encoding='latin1') as file:
    lines = file.readlines()

lines[:5]

data = [line.strip().split('@') for line in lines]

data[:5]

df = pd.DataFrame(data, columns=['Sentence', 'Label'])

df.head()

import matplotlib.pyplot as plt

labels_df = pd.DataFrame(df['Label'], columns=['Label'])

labels_df.head()

labels_df['Label'].value_counts().plot(kind='bar')

plt.title('Distribution of Labels')

plt.xlabel('Label')

plt.ylabel('Count')

plt.show()

labels_df['Label'].value_counts().plot(kind='bar')


plt.title('Distribution of Labels')
plt.xlabel('Label')
plt.ylabel('Count')

plt.show()

sentence_lengths = [len(sentence.split()) for sentence in df['Sentence']]

sentence_lengths[:5]

plt.hist(sentence_lengths, bins=30)

plt.title('Distribution of Sentence Lengths')
plt.xlabel('Sentence Length')
plt.ylabel('Frequency')

plt.hist(sentence_lengths, bins=30)
plt.title('Distribution of Sentence Lengths')
plt.xlabel('Sentence Length')
plt.ylabel('Frequency')
plt.show()

from collections import Counter
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

words = [word for sentence in df['Sentence'] for word in sentence.split()]

words[:10]

common_words = Counter(words).most_common(20)

common_words[:5]

common_words_df = pd.DataFrame(common_words, columns=['Word', 'Frequency'])

common_words_df.head()

sns.set_theme(style="whitegrid")

plt.figure(figsize=(10, 6))

sns.barplot(data=common_words_df, x='Frequency', y='Word', palette="magma")

plt.figure(figsize=(10, 6))
sns.barplot(data=common_words_df, x='Frequency', y='Word', palette="magma")

# Customize labels
plt.title('Top 20 Most Frequent Words in Text Data', fontsize=14, fontweight='bold')
plt.xlabel('Word Count', fontsize=12)
plt.ylabel('Word', fontsize=12)

# Display the plot
plt.show()

words = [word for sentence in df['Sentence'] for word in sentence.split()]

words[:10]

common_words = Counter(words).most_common(20)

common_words[:5]

positive_words = ['good', 'great', 'positive', 'profit', 'up', 'increase']
negative_words = ['bad', 'poor', 'negative', 'loss', 'down', 'decrease']

positive_counts = sum(sentence.lower().count(word) for sentence in df['Sentence'] for word in positive_words)
negative_counts = sum(sentence.lower().count(word) for sentence in df['Sentence'] for word in negative_words)

sns.barplot(x=['Positive Words', 'Negative Words'], y=[positive_counts, negative_counts], palette="coolwarm")

sns.barplot(x=['Positive Words', 'Negative Words'], y=[positive_counts, negative_counts], palette="coolwarm")

# Customize the plot labels
plt.title('Occurrence of Sentiment Words', fontsize=14, fontweight='bold')
plt.ylabel('Word Count', fontsize=12)

# Display the plot
plt.show()

import re

def clean_text(text):
    text = re.sub(r"[^\w\s]", '', text)  # Remove punctuation
    text = text.lower()  # Convert to lowercase
    text = re.sub(r"\s+", ' ', text)  # Collapse multiple spaces to one
    return text.strip()

df['Cleaned_Sentence'] = df['Sentence'].apply(clean_text)
df.head()

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df['Cleaned_Sentence'])
X

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize and apply TF-IDF Vectorizer
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df['Cleaned_Sentence'])
X

from sklearn.preprocessing import LabelEncoder

# Encode text labels into integers
encoder = LabelEncoder()
y = encoder.fit_transform(df['Label'])
y

from sklearn.model_selection import train_test_split

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.2,
                                                    random_state=1502)

X_train

X_train_dense = X_train.todense()
X_test_dense = X_test.todense()
X_train_dense

import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer with 64 neurons
    tf.keras.layers.Dropout(0.5),  # Dropout layer with 50% drop rate for regularization
    tf.keras.layers.Dense(3, activation='softmax')  # Output layer with 3 classes (softmax for multi-class classification)
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

history = model.fit(X_train_dense, y_train,
                    epochs=10,
                    batch_size=32,
                    validation_data=(X_test_dense, y_test))

loss, accuracy = model.evaluate(X_test_dense, y_test)

print(f'Loss: {loss}')
print(f'Accuracy: {accuracy}')

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='upper left')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(loc='upper left')

plt.tight_layout()
plt.show()

!pip install keras-tuner --upgrade --quiet

import tensorflow as tf
import keras_tuner as kt

# Define the HyperModel class for Keras Tuner
class SentimentHyperModel(kt.HyperModel):
    def build(self, hp):
        model = tf.keras.Sequential()

        # Input layer
        model.add(tf.keras.Input(shape=(X_train_dense.shape[1],)))

        # Hidden layer with tunable units and activation
        model.add(tf.keras.layers.Dense(
            units=hp.Int('units', min_value=64, max_value=512, step=64),
            activation=hp.Choice('activation', ['relu', 'tanh'])
        ))

        # Dropout with tunable rate
        model.add(tf.keras.layers.Dropout(
            rate=hp.Float('dropout_rate', 0.0, 0.5, step=0.1)
        ))

        # Output layer (3-class classification)
        model.add(tf.keras.layers.Dense(3, activation='softmax'))

        model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        return model

tuner = keras_tuner.RandomSearch(
    SentimentHyperModel(),
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=1,
    directory='my_tuner_dir',
    project_name='sentiment_tuning',
    overwrite=True
)

tuner.search(X_train_dense, y_train, epochs=10, validation_data=(X_test_dense, y_test))

best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"""
Best Number of Units: {best_hps.get('units')}
Best Activation Function: {best_hps.get('activation')}
Best Dropout Rate: {best_hps.get('dropout_rate')}
""")

def build_model_step2(hp):
    model = tf.keras.Sequential()

    # Add the first dense layer using the best parameters from step 1
    model.add(tf.keras.layers.Dense(
        units=tuner_step1.get_best_hyperparameters()[0].get('units'),
        activation=tuner_step1.get_best_hyperparameters()[0].get('activation'),
        input_shape=(X_train.shape[1],)
    ))

    # Apply dropout using the best rate from step 1
    model.add(tf.keras.layers.Dropout(
        rate=tuner_step1.get_best_hyperparameters()[0].get('dropout_rate')
    ))

    # Add a second dense layer where units are a new tunable hyperparameter
    model.add(tf.keras.layers.Dense(
        units=hp.Int('second_units', min_value=32, max_value=512, step=32),
        activation='relu'
    ))

    # Select optimizer and learning rate as new hyperparameters
    optimizer_selected = hp.Choice('optimizer', values=['adam', 'sgd'])
    learning_rate_selected = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])

    # Apply conditional logic for optimizer selection
    if optimizer_selected == 'adam':
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_selected)
    else:
        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate_selected)

    # Compile the model
    model.compile(
        optimizer=optimizer,
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

def build_model(hp):
    model = tf.keras.Sequential()

    # Tunable Dense layer
    model.add(tf.keras.layers.Dense(
        units=hp.Int('units', min_value=64, max_value=512, step=64),
        activation=hp.Choice('activation', values=['relu', 'tanh']),
        input_shape=(X_train.shape[1],)
    ))

    # Tunable Dropout
    model.add(tf.keras.layers.Dropout(
        rate=hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)
    ))

    # Output layer
    model.add(tf.keras.layers.Dense(3, activation='softmax'))

    # Compile the model
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

tuner_step1 = kt.RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=20,
    executions_per_trial=3,
    directory='tuner_step1_dir',
    project_name='step1_tuning'
)

tuner_step1.search(
    X_train_dense, y_train,
    epochs=10,
    validation_data=(X_test_dense, y_test)
)

tuner_step2 = kt.RandomSearch(
    build_model_step2,
    objective='val_accuracy',
    max_trials=20,
    executions_per_trial=3,
    directory='tuner_step2_directory',
    project_name='step2_tuning'
)

tuner_step2.search(
    X_train_dense, y_train,
    epochs=10,
    validation_data=(X_test_dense, y_test)
)

tuner_step2.results_summary()

def build_final_model(best_hps_step1, best_hps_step2):
    model = tf.keras.Sequential()

    # First Dense Layer with Best Hyperparameters from Step 1
    model.add(tf.keras.layers.Dense(
        units=best_hps_step1.get('units'),
        activation=best_hps_step1.get('activation'),
        input_shape=(X_train.shape[1],)
    ))

    # Dropout Layer with Best Rate from Step 1
    model.add(tf.keras.layers.Dropout(best_hps_step1.get('dropout_rate')))

    # Second Dense Layer with Best Units from Step 2
    model.add(tf.keras.layers.Dense(
        units=best_hps_step2.get('second_units'),
        activation='relu'
    ))

    # Select Optimizer and Learning Rate from Step 2
    optimizer_selected = best_hps_step2.get('optimizer')
    learning_rate_selected = best_hps_step2.get('learning_rate')

    # Conditional Optimizer Selection
    if optimizer_selected == 'adam':
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_selected)
    else:
        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate_selected)

    # Compile the Model
    model.compile(
        optimizer=optimizer,
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

best_hps_step1 = tuner_step1.get_best_hyperparameters()[0]
best_hps_step2 = tuner_step2.get_best_hyperparameters()[0]

final_model = build_final_model(best_hps_step1, best_hps_step2)

history_final = final_model.fit(
    X_train_dense, y_train,
    epochs=10,
    batch_size=32,
    validation_data=(X_test_dense, y_test)
)

final_loss, final_accuracy = final_model.evaluate(X_test_dense, y_test)

print(f'Final Model Loss: {final_loss}')
print(f'Final Model Accuracy: {final_accuracy}')

final_model.save('finalModel.h5')

print("Final model saved to 'finalModel.h5'")

import os
os.listdir()

!cp finalModel.h5 /content/

